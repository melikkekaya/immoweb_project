{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List, Dict\n",
    "from time import perf_counter\n",
    "\n",
    "start_time = perf_counter()\n",
    "all_data_file = \"C:/Users/Moura/git/immoweb_project/all_data7.csv\"\n",
    "\n",
    "header = [\n",
    "    \"Id\", 'Locality', 'Type of property', 'Subtype of property', 'Price', 'Type of sale',\n",
    "    'Number of rooms', 'Living Area', 'Fully equipped kitchen', 'Furnished', 'Open fire',\n",
    "    'Terrace', 'Garden', 'Surface area of the plot of land', 'Number of facades',\n",
    "    \"Swimming pool\", \"State of the building\", \"Url\"\n",
    "]\n",
    "\n",
    "def get_url_list() -> List:\n",
    "    root_url = \"https://www.immoweb.be/en/search/\"\n",
    "    estate_types = ['house', 'apartment']\n",
    "    all_immo_links = []\n",
    "    search_links = []\n",
    "\n",
    "    for estate in estate_types:\n",
    "        for page in range(1, 333):\n",
    "            url = f\"{root_url}{estate}/for-sale?countries=BE&page={page}&orderBy=relevance\"\n",
    "            search_links.append(url)\n",
    "\n",
    "    with ThreadPoolExecutor() as pool:\n",
    "        all_immo_links = list(pool.map(get_from_search_page, search_links))\n",
    "    \n",
    "    return [link for sublist in all_immo_links for link in sublist]\n",
    "\n",
    "def get_from_search_page(search_url):\n",
    "    req = requests.get(search_url)\n",
    "    soup = BeautifulSoup(req.content, 'html.parser')\n",
    "    card_results = soup.find_all('article', class_='card--result')\n",
    "    immo_links = []\n",
    "    for article in card_results:\n",
    "        link = article.find('a', class_='card__title-link')\n",
    "        if link:\n",
    "            immo_links.append(link['href'])\n",
    "    return immo_links\n",
    "\n",
    "def get_one_property_info(url_one_property: str) -> Dict:\n",
    "    req = requests.get(url_one_property)\n",
    "    soup = BeautifulSoup(req.content, 'html.parser')\n",
    "    try:\n",
    "        read_html_prop = pd.read_html(req.text)\n",
    "    except ValueError:  # no tables found\n",
    "        return {}\n",
    "\n",
    "    if len(read_html_prop) == 0:\n",
    "        return {}  # No tables found, return empty dictionary\n",
    "\n",
    "    # rest of your code\n",
    "\n",
    "    df_one_property = pd.concat(read_html_prop, ignore_index=True)\n",
    "    house_dict = {}\n",
    "\n",
    "    list_of_property_info = []\n",
    "    window_data = re.findall(\"window.dataLayer =(.+?);\\n\", req.text, re.S)\n",
    "    if window_data:\n",
    "        list_of_property_info.append(json.loads(window_data[0])[0]['classified'])\n",
    "\n",
    "    house_dict = list_of_property_info[0]\n",
    "\n",
    "    for i in df_one_property.index:\n",
    "        house_dict[df_one_property[0][i]] = df_one_property[1][i]\n",
    "\n",
    "    house_dict['Url'] = url_one_property  # Add the URL to the dictionary for each property\n",
    "\n",
    "    return house_dict\n",
    "\n",
    "def clean_data_to_csv(original_dict) -> Dict:\n",
    "    has_fireplace = int(original_dict.get('How many fireplaces?', 0))\n",
    "    living_area = original_dict.get('Living area')\n",
    "    living_area_value = living_area.split(' ', 1)[0] if living_area is not None else None\n",
    "\n",
    "    surface_of_plot = original_dict.get('Surface of the plot')\n",
    "    surface_of_plot_value = surface_of_plot.split(' ', 1)[0] if surface_of_plot is not None else None\n",
    "    new_dict = {\n",
    "        \"Id\": original_dict.get('id'),\n",
    "        'Locality': original_dict.get('Neighbourhood or locality'),\n",
    "        'Type of property': original_dict.get('type'),\n",
    "        'Subtype of property': original_dict.get('subtype'),\n",
    "     'Type of sale': original_dict.get('transactionType'),\n",
    "        'Number of rooms': original_dict.get('Bedrooms'),\n",
    "        'Living Area': living_area_value,\n",
    "        'Fully equipped kitchen': original_dict['kitchen']['type'],\n",
    "        'Furnished': True if original_dict.get('Furnished', '').lower() == 'yes' else False,\n",
    "        'Open fire': True if has_fireplace >= 1 else False,\n",
    "        'Terrace': False if original_dict['outdoor']['terrace']['exists'].lower() != 'true' else True,\n",
    "        'Garden': False if int(original_dict['outdoor']['garden']['surface'] or 0) < 1 else True,\n",
    "        'Surface area of the plot of land': surface_of_plot_value,\n",
    "        'Number of facades': original_dict.get('Number of frontages'),\n",
    "        \"Swimming pool\": original_dict['wellnessEquipment']['hasSwimmingPool'],\n",
    "        \"State of the building\": original_dict.get('Building condition'),\n",
    "        \"Url\": original_dict.get('Url')\n",
    "    }\n",
    "    return new_dict\n",
    "\n",
    "def adding_one_line_into_csv(new_dict):\n",
    "    with open(all_data_file, 'a', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=header)\n",
    "        writer.writerow(new_dict)\n",
    "\n",
    "def get_collective_data():\n",
    "    immo_links = get_url_list()\n",
    "\n",
    "    with ThreadPoolExecutor() as pool:\n",
    "        property_info_list = list(pool.map(get_one_property_info, immo_links))\n",
    "\n",
    "    with ThreadPoolExecutor() as pool:\n",
    "        clean_data_list = list(pool.map(clean_data_to_csv, property_info_list))\n",
    "\n",
    "    with ThreadPoolExecutor() as pool:\n",
    "        list(pool.map(adding_one_line_into_csv, clean_data_list))\n",
    "\n",
    "get_collective_data()\n",
    "\n",
    "print(\"Scraping completed.\")\n",
    "print(f\"\\nTime spent inside the loop: {perf_counter() - start_time} seconds.\")\n",
    "def print_csv_content(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(df)\n",
    "\n",
    "\n",
    "print_csv_content(all_data_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List, Dict\n",
    "from time import perf_counter\n",
    "\n",
    "start_time = perf_counter()\n",
    "all_data_file = \"all_data4.csv\"\n",
    "\n",
    "header = [\n",
    "    \"Id\", 'Locality', 'Type of property', 'Subtype of property', 'Price', 'Type of sale',\n",
    "    'Number of rooms', 'Living Area', 'Fully equipped kitchen', 'Furnished', 'Open fire',\n",
    "    'Terrace', 'Garden', 'Surface area of the plot of land', 'Number of facades',\n",
    "    \"Swimming pool\", \"State of the building\", \"Url\"\n",
    "]\n",
    "\n",
    "def get_url_list() -> List:\n",
    "    root_url = \"https://www.immoweb.be/en/search/\"\n",
    "    estate_types = ['house', 'apartment']\n",
    "    all_immo_links = []\n",
    "    search_links = []\n",
    "\n",
    "    for estate in estate_types:\n",
    "        for page in range(1, 333):\n",
    "            url = f\"{root_url}{estate}/for-sale?countries=BE&page={page}&orderBy=relevance\"\n",
    "            search_links.append(url)\n",
    "\n",
    "    with ThreadPoolExecutor() as pool:\n",
    "        all_immo_links = pool.map(get_from_search_page, search_links)\n",
    "    \n",
    "    return [link for sublist in all_immo_links for link in sublist]\n",
    "\n",
    "def get_from_search_page(search_url):\n",
    "    req = requests.get(search_url)\n",
    "    soup = BeautifulSoup(req.content, 'html.parser')\n",
    "    card_results = soup.find_all('article', class_='card--result')\n",
    "    immo_links = []\n",
    "    for article in card_results:\n",
    "        link = article.find('a', class_='card__title-link')\n",
    "        if link:\n",
    "            immo_links.append(link['href'])\n",
    "    return immo_links\n",
    "\n",
    "def get_one_property_info(url_one_property: str) -> Dict:\n",
    "    req = requests.get(url_one_property)\n",
    "    soup = BeautifulSoup(req.content, 'html.parser')\n",
    "    read_html_prop = pd.read_html(req.text)\n",
    "    \n",
    "    if len(read_html_prop) == 0:\n",
    "        return {}  # No tables found, return empty dictionary\n",
    "    \n",
    "    df_one_property = pd.concat(read_html_prop, ignore_index=True)\n",
    "    house_dict = {}\n",
    "\n",
    "    list_of_property_info = []\n",
    "    window_data = re.findall(\"window.dataLayer =(.+?);\\n\", req.text, re.S)\n",
    "    if window_data:\n",
    "        list_of_property_info.append(json.loads(window_data[0])[0]['classified'])\n",
    "\n",
    "    house_dict = list_of_property_info[0]\n",
    "\n",
    "    for i in df_one_property.index:\n",
    "        house_dict[df_one_property[0][i]] = df_one_property[1][i]\n",
    "\n",
    "    house_dict['url'] = url_one_property  # Add the URL to the dictionary for each property\n",
    "\n",
    "    return house_dict\n",
    "\n",
    "\n",
    "\n",
    "def clean_data_to_csv(original_dict) -> Dict:\n",
    "    has_fireplace = int(original_dict.get('How many fireplaces?', 0))\n",
    "    living_area = original_dict.get('Living area')\n",
    "    living_area_value = living_area.split(' ', 1)[0] if living_area is not None else None\n",
    "\n",
    "    surface_of_plot = original_dict.get('Surface of the plot')\n",
    "    surface_of_plot_value = surface_of_plot.split(' ', 1)[0] if surface_of_plot is not None else None\n",
    "\n",
    "    new_dict = {\n",
    "        \"Id\": original_dict.get('id'),\n",
    "        'Locality': original_dict.get('Neighbourhood or locality'),\n",
    "        'Type of property': original_dict.get('type'),\n",
    "        'Subtype of property': original_dict.get('subtype'),\n",
    "        'Price': original_dict.get('price'),\n",
    "        'Type of sale': original_dict.get('transactionType'),\n",
    "        'Number of rooms': original_dict.get('Bedrooms'),\n",
    "        'Living Area': living_area_value,\n",
    "        'Fully equipped kitchen': original_dict['kitchen']['type'],\n",
    "        'Furnished': True if original_dict.get('Furnished', '').lower() == 'yes' else False,\n",
    "        'Open fire': True if has_fireplace >= 1 else False,\n",
    "        'Terrace': False if original_dict['outdoor']['terrace']['exists'].lower() != 'true' else True,\n",
    "        'Garden': False if int(original_dict['outdoor']['garden']['surface'] or 0) < 1 else True,\n",
    "        'Surface area of the plot of land': surface_of_plot_value,\n",
    "        'Number of facades': original_dict.get('Number of frontages'),\n",
    "        \"Swimming pool\": original_dict['wellnessEquipment']['hasSwimmingPool'],\n",
    "        \"State of the building\": original_dict.get('Building condition'),\n",
    "        \"Url\": original_dict.get('url')\n",
    "    }\n",
    "    return new_dict\n",
    "\n",
    "def adding_one_line_into_csv(new_dict):\n",
    "    with open(all_data_file, 'a', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=header)\n",
    "        writer.writerow(new_dict)\n",
    "\n",
    "def get_collective_data():\n",
    "    immo_links = get_url_list()\n",
    "\n",
    "    with ThreadPoolExecutor() as pool:\n",
    "        property_info_list = list(pool.map(get_one_property_info, immo_links))\n",
    "\n",
    "    for property_info in property_info_list:\n",
    "        clean_data = clean_data_to_csv(property_info)\n",
    "        adding_one_line_into_csv(clean_data)\n",
    "\n",
    "get_collective_data()\n",
    "\n",
    "print(\"Scraping completed.\")\n",
    "print(f\"\\nTime spent inside the loop: {perf_counter() - start_time} seconds.\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List, Dict\n",
    "from time import perf_counter\n",
    "\n",
    "start_time = perf_counter()\n",
    "all_data_file = \"all_data.csv\"\n",
    "\n",
    "def get_url_list() -> List:\n",
    "    root_url = \"https://www.immoweb.be/en/search/\"\n",
    "    estate_types = ['house', 'apartment']\n",
    "    all_immo_links = []\n",
    "    search_links = []\n",
    "\n",
    "    for estate in estate_types:\n",
    "        for page in range(1, 11):\n",
    "            url = f\"{root_url}{estate}/for-sale?countries=BE&page={page}&orderBy=relevance\"\n",
    "            search_links.append(url)\n",
    "            print(page)\n",
    "\n",
    "    with ThreadPoolExecutor() as pool:\n",
    "        all_immo_links = pool.map(get_from_search_page, search_links)\n",
    "    \n",
    "    return [link for sublist in all_immo_links for link in sublist]\n",
    "\n",
    "def get_from_search_page(search_url):\n",
    "    req = requests.get(search_url)\n",
    "    soup = BeautifulSoup(req.content, 'html.parser')\n",
    "    card_results = soup.find_all('article', class_='card--result')\n",
    "    immo_links = []\n",
    "    for article in card_results:\n",
    "        link = article.find('a', class_='card__title-link')\n",
    "        if link:\n",
    "            immo_links.append(link['href'])\n",
    "    return immo_links\n",
    "\n",
    "def get_one_property_info(url_one_property: str) -> Dict:\n",
    "    req = requests.get(url_one_property)\n",
    "    read_html_prop = pd.read_html(req.text)\n",
    "    df_one_property = pd.concat(read_html_prop, ignore_index=True)\n",
    "\n",
    "    list_of_property_info = []\n",
    "    window_data = re.findall(\"window.dataLayer =(.+?);\\n\", req.text, re.S)\n",
    "    if window_data:\n",
    "        list_of_property_info.append(json.loads(window_data[0])[0]['classified'])\n",
    "\n",
    "    house_dict = list_of_property_info[0]\n",
    "\n",
    "    for i in df_one_property.index:\n",
    "        house_dict[df_one_property[0][i]] = df_one_property[1][i]\n",
    "\n",
    "    return house_dict\n",
    "\n",
    "def clean_data_to_csv(original_dict) -> Dict:\n",
    "    has_fireplace = int(original_dict.get('How many fireplaces?', 0))\n",
    "    living_area = original_dict.get('Living area')\n",
    "    living_area_value = living_area.split(' ', 1)[0] if living_area is not None else None\n",
    "\n",
    "    surface_of_plot = original_dict.get('Surface of the plot')\n",
    "    surface_of_plot_value = surface_of_plot.split(' ', 1)[0] if surface_of_plot is not None else None\n",
    "\n",
    "    new_dict = {\n",
    "        \"Id\": original_dict.get('id'),\n",
    "        'Locality': original_dict.get('Neighbourhood or locality'),\n",
    "        'Type of property': original_dict.get('type'),\n",
    "        'Subtype of property': original_dict.get('subtype'),\n",
    "        'Price': original_dict.get('price'),\n",
    "        'Type of sale': original_dict.get('transactionType'),\n",
    "        'Number of rooms': original_dict.get('Bedrooms'),\n",
    "        'Living Area': living_area_value,\n",
    "        'Fully equipped kitchen': original_dict['kitchen']['type'],\n",
    "        'Furnished': True if original_dict.get('Furnished', '').lower() == 'yes' else False,\n",
    "        'Open fire': True if has_fireplace >= 1 else False,\n",
    "        'Terrace': False if original_dict['outdoor']['terrace']['exists'].lower() != 'true' else True,\n",
    "        'Garden': False if int(original_dict['outdoor']['garden']['surface'] or 0) < 1 else True,\n",
    "        'Surface area of the plot of land': surface_of_plot_value,\n",
    "        'Number of facades': original_dict.get('Number of frontages'),\n",
    "        \"Swimming pool\": original_dict['wellnessEquipment']['hasSwimmingPool'],\n",
    "        \"State of the building\": original_dict.get('Building condition'),\n",
    "        \"Url\": original_dict.get('url')\n",
    "    }\n",
    "    return new_dict\n",
    "\n",
    "def adding_one_line_into_csv(new_dict):\n",
    "    header = [\n",
    "        \"Id\", 'Locality', 'Type of property', 'Subtype of property', 'Price', 'Type of sale',\n",
    "        'Number of rooms', 'Living Area', 'Fully equipped kitchen', 'Furnished', 'Open fire',\n",
    "        'Terrace', 'Garden', 'Surface area of the plot of land', 'Number of facades',\n",
    "        \"Swimming pool\", \"State of the building\", \"Url\"\n",
    "    ]\n",
    "    with open(all_data_file, 'a') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=header)\n",
    "        writer.writerow(new_dict)\n",
    "\n",
    "def get_collective_data():\n",
    "    immo_links = get_url_list()\n",
    "\n",
    "    with ThreadPoolExecutor() as pool:\n",
    "        property_info_list = list(pool.map(get_one_property_info, immo_links))\n",
    "\n",
    "    for property_info in property_info_list:\n",
    "        clean_data = clean_data_to_csv(property_info)\n",
    "        adding_one_line_into_csv(clean_data)\n",
    "\n",
    "get_collective_data()\n",
    "\n",
    "print(\"Scraping completed.\")\n",
    "print(f\"\\nTime spent inside the loop: {perf_counter() - start_time} seconds.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List, Dict\n",
    "from time import perf_counter\n",
    "from requests_html import HTMLSession\n",
    "\n",
    "\n",
    "start_time = perf_counter()\n",
    "all_data_file = \"all_data5.csv\"\n",
    "\n",
    "header = [\n",
    "    \"Id\", 'Locality', 'Type of property', 'Subtype of property', 'Price', 'Type of sale',\n",
    "    'Number of rooms', 'Living Area', 'Fully equipped kitchen', 'Furnished', 'Open fire',\n",
    "    'Terrace', 'Garden', 'Surface area of the plot of land', 'Number of facades',\n",
    "    \"Swimming pool\", \"State of the building\", \"Url\"\n",
    "]\n",
    "\n",
    "\n",
    "def get_url_list() -> List:\n",
    "    root_url = \"https://www.immoweb.be/en/search/\"\n",
    "    estate_types = ['house', 'apartment']\n",
    "    search_links = []\n",
    "\n",
    "    for estate in estate_types:\n",
    "        for page in range(1, 11):\n",
    "            url = f\"{root_url}{estate}/for-sale?countries=BE&page={page}&orderBy=relevance\"\n",
    "            search_links.append(url)\n",
    "\n",
    "    return search_links\n",
    "\n",
    "\n",
    "def get_from_search_page(search_url):\n",
    "    session = HTMLSession()\n",
    "    response = session.get(search_url)\n",
    "    response.html.render()  # Render JavaScript content\n",
    "\n",
    "    card_results = response.html.find('article.card--result')\n",
    "    immo_links = []\n",
    "\n",
    "    for article in card_results:\n",
    "        link = article.find('a.card__title-link', first=True)\n",
    "        if link:\n",
    "            immo_links.append(link.attrs['href'])\n",
    "\n",
    "    return immo_links\n",
    "\n",
    "\n",
    "def get_one_property_info(url_one_property: str) -> Dict:\n",
    "    session = HTMLSession()\n",
    "    response = session.get(url_one_property)\n",
    "    response.html.render()  # Render JavaScript content\n",
    "\n",
    "    read_html_prop = pd.read_html(response.html.html)\n",
    "    df_one_property = pd.concat(read_html_prop, ignore_index=True)\n",
    "\n",
    "    list_of_property_info = []\n",
    "    window_data = response.html.search(\"window.dataLayer = {};{}\", first=True)\n",
    "    if window_data:\n",
    "        list_of_property_info.append(json.loads(window_data[0])['classified'])\n",
    "\n",
    "    house_dict = list_of_property_info[0]\n",
    "\n",
    "    for i in df_one_property.index:\n",
    "        house_dict[df_one_property[0][i]] = df_one_property[1][i]\n",
    "\n",
    "    return house_dict\n",
    "\n",
    "\n",
    "def clean_data_to_csv(original_dict) -> Dict:\n",
    "    has_fireplace = int(original_dict.get('How many fireplaces?', 0))\n",
    "    living_area = original_dict.get('Living area')\n",
    "    living_area_value = living_area.split(' ', 1)[0] if living_area is not None else None\n",
    "\n",
    "    surface_of_plot = original_dict.get('Surface of the plot')\n",
    "    surface_of_plot_value = surface_of_plot.split(' ', 1)[0] if surface_of_plot is not None else None\n",
    "\n",
    "    new_dict = {\n",
    "        \"Id\": original_dict.get('id'),\n",
    "        'Locality': original_dict.get('Neighbourhood or locality'),\n",
    "        'Type of property': original_dict.get('type'),\n",
    "        'Subtype of property': original_dict.get('subtype'),\n",
    "        'Price': original_dict.get('price'),\n",
    "        'Type of sale': original_dict.get('transactionType'),\n",
    "        'Number of rooms': original_dict.get('Bedrooms'),\n",
    "        'Living Area': living_area_value,\n",
    "        'Fully equipped kitchen': original_dict['kitchen']['type'],\n",
    "        'Furnished': True if original_dict.get('Furnished', '').lower() == 'yes' else False,\n",
    "        'Open fire': True if has_fireplace >= 1 else False,\n",
    "        'Terrace': False if original_dict['outdoor']['terrace']['exists'].lower() != 'true' else True,\n",
    "        'Garden': False if int(original_dict['outdoor']['garden']['surface'] or 0) < 1 else True,\n",
    "        'Surface area of the plot of land': surface_of_plot_value,\n",
    "        'Number of facades': original_dict.get('Number of frontages'),\n",
    "        \"Swimming pool\": original_dict['wellnessEquipment']['hasSwimmingPool'],\n",
    "        \"State of the building\": original_dict.get('Building condition'),\n",
    "        \"Url\": original_dict.get('url')\n",
    "    }\n",
    "    return new_dict\n",
    "\n",
    "\n",
    "def adding_one_line_into_csv(new_dict):\n",
    "    with open(all_data_file, 'a', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=header)\n",
    "        writer.writerow(new_dict)\n",
    "\n",
    "\n",
    "def get_collective_data():\n",
    "    search_links = get_url_list()\n",
    "    all_immo_links = []\n",
    "\n",
    "    with ThreadPoolExecutor() as pool:\n",
    "        all_immo_links = list(pool.map(get_from_search_page, search_links))\n",
    "\n",
    "    immo_links = [link for sublist in all_immo_links for link in sublist]\n",
    "\n",
    "    with ThreadPoolExecutor() as pool:\n",
    "        property_info_list = list(pool.map(get_one_property_info, immo_links))\n",
    "\n",
    "    for property_info in property_info_list:\n",
    "        clean_data = clean_data_to_csv(property_info)\n",
    "        adding_one_line_into_csv(clean_data)\n",
    "\n",
    "\n",
    "get_collective_data()\n",
    "\n",
    "print(\"Scraping completed.\")\n",
    "print(f\"\\nTime spent inside the loop: {perf_counter() - start_time} seconds.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed.\n",
      "\n",
      "Time spent inside the loop: 14.359548362999703 seconds.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List, Dict\n",
    "from time import perf_counter\n",
    "\n",
    "start_time = perf_counter()\n",
    "all_data_file = \"all_data.csv\"\n",
    "\n",
    "def get_url_list() -> List:\n",
    "    root_url = \"https://www.immoweb.be/en/search/\"\n",
    "    estate_types = ['house', 'apartment']\n",
    "    all_immo_links = []\n",
    "    search_links = []\n",
    "\n",
    "    for estate in estate_types:\n",
    "        for page in range(1, 10):  # for testing purpose I changed from 334 to 2\n",
    "            url = f\"{root_url}{estate}/for-sale?countries=BE&page={page}&orderBy=relevance\"\n",
    "            search_links.append(url)\n",
    "\n",
    "    with ThreadPoolExecutor() as pool:\n",
    "        all_immo_links = list(pool.map(get_from_search_page, search_links))\n",
    "    return all_immo_links[0]\n",
    "\n",
    "def get_from_search_page(search_url):\n",
    "\n",
    "    req = requests.get(search_url)\n",
    "    soup = BeautifulSoup(req.content, 'html.parser')\n",
    "    card_results = soup.find_all('article', class_='card--result')\n",
    "    immo_links = []\n",
    "    for article in card_results:\n",
    "        link = article.find('a', class_='card__title-link')\n",
    "        if link:\n",
    "            immo_links.append(link['href'])\n",
    "    return immo_links\n",
    "\n",
    "def get_one_property_info(url_one_property: str) -> Dict:\n",
    "    req = requests.get(url_one_property)\n",
    "    # print(req.status_code)\n",
    "    read_html_prop = pd.read_html(req.text)\n",
    "    df_one_property = pd.concat(read_html_prop, ignore_index=True)\n",
    "\n",
    "    list_of_property_info = []\n",
    "    window_data = re.findall(\"window.dataLayer =(.+?);\\n\", req.text, re.S)\n",
    "    if window_data:\n",
    "        list_of_property_info.append(json.loads(window_data[0])[0]['classified'])\n",
    "\n",
    "    house_dict = list_of_property_info[0]\n",
    "\n",
    "    for i in df_one_property.index:\n",
    "        house_dict[df_one_property[0][i]] = df_one_property[1][i]\n",
    "\n",
    "    return house_dict\n",
    "\n",
    "def clean_data_to_csv(original_dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Take info from get_one_prop_info() as JSON or dict,\n",
    "    and add them to a CSV for only one property.\n",
    "    \"\"\"\n",
    "\n",
    "    has_fireplace = int(original_dict.get('How many fireplaces?', 0))\n",
    "    living_area = original_dict.get('Living area')\n",
    "    living_area_value = living_area.split(' ', 1)[0] if living_area is not None else None\n",
    "\n",
    "    surface_of_plot = original_dict.get('Surface of the plot')\n",
    "    surface_of_plot_value = surface_of_plot.split(' ', 1)[0] if surface_of_plot is not None else None\n",
    "\n",
    "    new_dict = {\n",
    "        \"Id\": original_dict.get('id'),\n",
    "        'Locality': original_dict.get('Neighbourhood or locality'),\n",
    "        'Type of property': original_dict.get('type'),\n",
    "        'Subtype of property': original_dict.get('subtype'),\n",
    "        'Price': original_dict.get('price'),\n",
    "        'Type of sale': original_dict.get('transactionType'),\n",
    "        'Number of rooms': original_dict.get('Bedrooms'),\n",
    "        'Living Area': living_area_value,\n",
    "        'Fully equipped kitchen': original_dict['kitchen']['type'],\n",
    "        'Furnished': True if original_dict.get('Furnished', '').lower() == 'yes' else False,\n",
    "        'Open fire': True if has_fireplace >= 1 else False,\n",
    "        'Terrace': False if original_dict['outdoor']['terrace']['exists'].lower() != 'true' else True,\n",
    "        'Garden': False if int(original_dict['outdoor']['garden']['surface'] or 0) < 1 else True,\n",
    "        'Surface area of the plot of land': surface_of_plot_value,\n",
    "        'Number of facades': original_dict.get('Number of frontages'),\n",
    "        \"Swimming pool\": original_dict['wellnessEquipment']['hasSwimmingPool'],\n",
    "        \"State of the building\": original_dict.get('Building condition'),\n",
    "        \"Url\": original_dict.get('url')\n",
    "    }\n",
    "    return new_dict\n",
    "                                                            \n",
    "def adding_one_line_into_csv(new_dict):\n",
    "    header = [\n",
    "        \"Id\", 'Locality', 'Type of property', 'Subtype of property', 'Price', 'Type of sale',\n",
    "        'Number of rooms', 'Living Area', 'Fully equipped kitchen', 'Furnished', 'Open fire',\n",
    "        'Terrace', 'Garden', 'Surface area of the plot of land', 'Number of facades',\n",
    "        \"Swimming pool\", \"State of the building\", \"Url\"\n",
    "    ]\n",
    "    with open(all_data_file, 'w') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=header)\n",
    "        writer.writeheader()\n",
    "        writer.writerow(new_dict)\n",
    "\n",
    "def get_collective_data():\n",
    "    \"\"\"\n",
    "    clean_data_to_csv() for all URLs from get_url_list() using pool\n",
    "    \"\"\"\n",
    "    immo_links = get_url_list()\n",
    "\n",
    "    with ThreadPoolExecutor() as pool:\n",
    "        # Map get_one_property_info function to each link\n",
    "        property_info_list = list(pool.map(get_one_property_info, immo_links))\n",
    "\n",
    "    for property_info in property_info_list:\n",
    "        clean_data = clean_data_to_csv(property_info)\n",
    "        adding_one_line_into_csv(clean_data)\n",
    "\n",
    "get_collective_data()\n",
    "\n",
    "print(\"Scraping completed.\")\n",
    "print(f\"\\nTime spent inside the loop: {perf_counter() - start_time} seconds.\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "root_url = \"https://www.immoweb.be/en/search/\"\n",
    "distribution_type = \"BUY\"\n",
    "estate_type = \"HOUSE\"\n",
    "# need to change in  a list \n",
    "# estate_type =['house', 'apartment'] \n",
    "country = \"BE\"\n",
    "\n",
    "search_url = f\"{root_url}{distribution_type}/{estate_type}?countries={country}\"\n",
    "print(search_url)\n",
    "response = requests.get(search_url)\n",
    "html_content = response.text\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "house_listings = soup.find_all(\"a\", class_=\"card__title-link\")\n",
    "house_urls = [listing[\"href\"] for listing in house_listings]\n",
    "for url in house_urls:\n",
    "    print(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\"house/for-sale?countries=BE&priceType=SALE_PRICE&page=1&orderBy=relevance\"\n",
    "\"https://www.immoweb.be/en/search/house/for-sale?countries=BE&priceType=SALE_PRICE&page=333&orderBy=relevance\"\n",
    "root_url = \"https://www.immoweb.be/en/search/\"\n",
    "estate_types = ['house', 'apartment']\n",
    "max_page = 333  # Set the maximum page number to 333\n",
    "immo_link = []\n",
    "for estate in estate_types:\n",
    "    page = 1\n",
    "    while page <= max_page:\n",
    "        url = f\"{root_url}{estate}/for-sale?countries=BE&page={page}&orderBy=relevance\"\n",
    "        req = requests.get(url)\n",
    "        print(\"Page: \", page)\n",
    "        print(\"Status Code:\", req.status_code)\n",
    "\n",
    "        soup = BeautifulSoup(req.content, 'html.parser')\n",
    "        card_results = soup.find_all('article', class_='card--result')\n",
    "\n",
    "        href_links = []\n",
    "\n",
    "        for article in card_results:\n",
    "            link = article.find('a', class_='card__title-link')\n",
    "            if link:\n",
    "                href_links.append(link['href'])\n",
    "\n",
    "        for i, link in enumerate(href_links, 1):\n",
    "            immo_link = immo_link.append(link)\n",
    "        page += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import threading\n",
    "from time import perf_counter\n",
    "\n",
    "start_time = perf_counter()\n",
    "root_url = \"https://www.immoweb.be/en/search/\"\n",
    "estate_types = ['house', 'apartment']\n",
    "max_page = 333  # Set the maximum page number to 333\n",
    "immo_link = []\n",
    "\n",
    "# Function to retrieve and parse data from a URL\n",
    "def retrieve_data(url):\n",
    "    response = requests.get(url) \n",
    "    js_content = response.text\n",
    "\n",
    "    # Extract the content within the window.dataLayer assignment using regex\n",
    "    match = re.search(r'window\\.dataLayer\\s*=\\s*(\\[.*?\\]);', js_content)\n",
    "\n",
    "    if match:\n",
    "        data = match.group(1)  # Get the matched content within the first capturing group\n",
    "        data_dict = json.loads(data)  # Parse the content into a Python dictionary\n",
    "        \n",
    "        # Replace empty values with 'none'\n",
    "        def replace_empty_with_none(obj):\n",
    "            for key, value in obj.items():\n",
    "                if isinstance(value, dict):\n",
    "                    replace_empty_with_none(value)\n",
    "                elif isinstance(value, str) and not value:\n",
    "                    obj[key] = 'none'\n",
    "        \n",
    "        replace_empty_with_none(data_dict)\n",
    "        \n",
    "        with lock:\n",
    "            data_list.append(data_dict)\n",
    "\n",
    "lock = threading.Lock() \n",
    "data_list = []\n",
    "\n",
    "for estate in estate_types:\n",
    "    page = 1\n",
    "    while page <= max_page:\n",
    "        url = f\"{root_url}{estate}/for-sale?countries=BE&page={page}&orderBy=relevance\"\n",
    "        req = requests.get(url)\n",
    "        print(\"Page:\", page)\n",
    "        print(\"Status Code:\", req.status_code)\n",
    "\n",
    "        soup = BeautifulSoup(req.content, 'html.parser')\n",
    "        card_results = soup.find_all('article', class_='card--result')\n",
    "\n",
    "        href_links = []\n",
    "\n",
    "        for article in card_results:\n",
    "            link = article.find('a', class_='card__title-link')\n",
    "            if link:\n",
    "                href_links.append(link['href'])\n",
    "\n",
    "        for link in href_links:\n",
    "            immo_link.append(link)\n",
    "            # print(\"URL:\", link)\n",
    "            # Retrieve and parse the data from the URL using threading\n",
    "            thread = threading.Thread(target=retrieve_data, args=(link,))\n",
    "            thread.start()\n",
    "\n",
    "        page += 1\n",
    "\n",
    "print(\"Scraping completed.\")\n",
    "for thread in threading.enumerate():\n",
    "    if thread != threading.current_thread():\n",
    "        thread.join()\n",
    "\n",
    "# Print the parsed data\n",
    "# for index, data_dict in enumerate(data_list):\n",
    "    # print(f\"Data from URL {immo_link[index]}:\")\n",
    "    # print(data_dict)\n",
    "    # print()\n",
    "\n",
    "print(f\"\\nTime spent inside the loop: {perf_counter() - start_time} seconds.\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import perf_counter\n",
    "from json import loads\n",
    "\n",
    "start_time = perf_counter()\n",
    "root_url = \"https://www.immoweb.be/en/search/\"\n",
    "estate_types = ['house', 'apartment']\n",
    "max_page = 333  # Set the maximum page number to 333\n",
    "immo_link = []\n",
    "data_dict =  []\n",
    "\n",
    "url = \"https://www.immoweb.be/en/classified/apartment-block/for-sale/henri-chapelle/4841/10130670\"\n",
    "# Function to retrieve and parse data from a URL\n",
    "def retrieve_data(url):\n",
    "        # print(url)\n",
    "        response = requests.get(url)\n",
    "        js_content = response.text\n",
    "        # print(js_content)\n",
    "\n",
    "        # Extract the content within the window.dataLayer assignment using regex\n",
    "        match = re.search(r'window\\.dataLayer\\s*=\\s*(\\[.*?\\]);', js_content)\n",
    "        if match:\n",
    "            data = match.group(1)  # Get the matched content within the first capturing group\n",
    "            data_dict = json.loads(data)  # Parse the content into a Python dictionary\n",
    "\n",
    "            soup = BeautifulSoup(requests.get(url).content)\n",
    "            script_text = soup.find(\"script\", text=re.compile(\"var\\s+dataLayer\")).text.split(\"= \", 1)[1]\n",
    "            json_data = loads(script_text[:script_text.find(\";\")])\n",
    "            print(\"json_data\")\n",
    "            print(json_data)\n",
    "\n",
    "            # print(data_dict)\n",
    "            # Replace empty values with 'none'\n",
    "            def replace_empty_with_none(dict_to_clean):\n",
    "                for key, value in dict_to_clean.items():\n",
    "                    if isinstance(value, dict):\n",
    "                        replace_empty_with_none(value)\n",
    "                    elif isinstance(value, str) and not value:\n",
    "                        dict_to_clean[key] = 'none'\n",
    "            replace_empty_with_none(data_dict)\n",
    "\n",
    "            with lock:\n",
    "                data_list.append(data_dict)\n",
    "retrieve_data(url)\n",
    "print(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Nepal': 'none', 'Italy': 'Rome', 'England': 'London'}\n"
     ]
    }
   ],
   "source": [
    "example_dict = {\"Nepal\": \"\", \"Italy\": \"Rome\", \"England\": \"London\"}\n",
    "\n",
    "def replace_empty_with_none(dict_to_clean):\n",
    "        for key, value in dict_to_clean.items():\n",
    "            if isinstance(value, dict):\n",
    "                replace_empty_with_none(value)\n",
    "            elif isinstance(value, str) and not value:\n",
    "                dict_to_clean[key] = 'none'\n",
    "\n",
    "\n",
    "replace_empty_with_none(example_dict)\n",
    "print(example_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_url_list():\n",
    "    for estate in estate_types:\n",
    "        page = 1\n",
    "        while page <= max_page:\n",
    "            url = f\"{root_url}{estate}/for-sale?countries=BE&page={page}&orderBy=relevance\"\n",
    "            req = requests.get(url)\n",
    "            soup = BeautifulSoup(req.content, 'html.parser')\n",
    "            card_results = soup.find_all('article', class_='card--result')\n",
    "            immo_links = []\n",
    "            for article in card_results:\n",
    "                link = article.find('a', class_='card__title-link')\n",
    "                if link:\n",
    "                    immo_links.append(link['href'])\n",
    "            page += 1\n",
    "    print(immo_links)\n",
    "\n",
    "get_url_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m immo_links \u001b[39m=\u001b[39m []\n\u001b[1;32m     50\u001b[0m \u001b[39mwith\u001b[39;00m Pool() \u001b[39mas\u001b[39;00m pool:\n\u001b[0;32m---> 51\u001b[0m     immo_links \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39;49mmap(get_url_list, estate_types)\n\u001b[1;32m     52\u001b[0m     \u001b[39mprint\u001b[39m(immo_links)\n\u001b[1;32m     54\u001b[0m immo_dicts \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m, func, iterable, chunksize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[39m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[39m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_async(func, iterable, mapstar, chunksize)\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    769\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event\u001b[39m.\u001b[39;49mwait(timeout)\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    608\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import concurrent.futures\n",
    "from multiprocessing import Pool\n",
    "import json\n",
    "from time import perf_counter\n",
    "\n",
    "root_url = \"https://www.immoweb.be/en/search/\"\n",
    "estate_types = ['house', 'apartment']\n",
    "max_page = 333  # Set the maximum page number to 333\n",
    "\n",
    "def get_url_list(estate):\n",
    "    immo_links = []\n",
    "    page = 1\n",
    "    while page <= max_page:\n",
    "        url = f\"{root_url}{estate}/for-sale?countries=BE&page={page}&orderBy=relevance\"\n",
    "        req = requests.get(url)\n",
    "        soup = BeautifulSoup(req.content, 'html.parser')\n",
    "        card_results = soup.find_all('article', class_='card--result')\n",
    "        for article in card_results:\n",
    "            link = article.find('a', class_='card__title-link')\n",
    "            if link:\n",
    "                immo_links.append(link['href'])\n",
    "        page += 1\n",
    "    return immo_links\n",
    "\n",
    "def get_immo_dict(link):\n",
    "    req = requests.get(link)\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "    script_tags = soup.find_all('script')\n",
    "    second_script = script_tags[1]\n",
    "    script_content = second_script.string\n",
    "    new_script_content = script_content.split('\"classified\": ')[1]\n",
    "    new_new_cont = new_script_content.split(\"\"\",\n",
    "                                    \"customer\": \"\"\")[0]\n",
    "    dict1 = json.loads(new_new_cont)\n",
    "    return dict1\n",
    "\n",
    "def replace_empty_with_none(dict_to_clean):\n",
    "    for key, value in dict_to_clean.items():\n",
    "        if isinstance(value, dict):\n",
    "            replace_empty_with_none(value)\n",
    "        elif isinstance(value, str) and not value:\n",
    "            dict_to_clean[key] = None\n",
    "    return dict_to_clean\n",
    "\n",
    "start_time = perf_counter()\n",
    "\n",
    "immo_links = []\n",
    "with Pool() as pool:\n",
    "    immo_links = pool.map(get_url_list, estate_types)\n",
    "    print(immo_links)\n",
    "\n",
    "immo_dicts = []\n",
    "with Pool() as pool:\n",
    "    results = pool.map(get_immo_dict, immo_links)\n",
    "    print(immo_dicts)\n",
    "    for result in results:\n",
    "        result = replace_empty_with_none(result)\n",
    "        immo_dicts.append(result)\n",
    "\n",
    "with open('immo_dump.json', 'w') as outfile:\n",
    "    json.dump(immo_dicts, outfile, indent=4)\n",
    "\n",
    "print(\"Scraping completed\")\n",
    "print(f\"\\nTime spent inside the loop: {perf_counter() - start_time} seconds.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function :\n",
    "    get_url_list\n",
    "        list(url_list)\n",
    "\n",
    "    get_immo_dict(url_list)\n",
    "        list(immo_dict) or json\n",
    "    \n",
    "    replace_empty_with_none(immo_dict)\n",
    "        list(cleaned_immo_dict) or json\n",
    "        \n",
    "    get_csv_file(immo_dict)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
