{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "root_url = \"https://www.immoweb.be/en/search/\"\n",
    "distribution_type = \"BUY\"\n",
    "estate_type = \"HOUSE\"\n",
    "# need to change in  a list \n",
    "# estate_type =['house', 'apartment'] \n",
    "country = \"BE\"\n",
    "\n",
    "search_url = f\"{root_url}{distribution_type}/{estate_type}?countries={country}\"\n",
    "print(search_url)\n",
    "response = requests.get(search_url)\n",
    "html_content = response.text\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "house_listings = soup.find_all(\"a\", class_=\"card__title-link\")\n",
    "house_urls = [listing[\"href\"] for listing in house_listings]\n",
    "for url in house_urls:\n",
    "    print(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\"house/for-sale?countries=BE&priceType=SALE_PRICE&page=1&orderBy=relevance\"\n",
    "\"https://www.immoweb.be/en/search/house/for-sale?countries=BE&priceType=SALE_PRICE&page=333&orderBy=relevance\"\n",
    "root_url = \"https://www.immoweb.be/en/search/\"\n",
    "estate_types = ['house', 'apartment']\n",
    "max_page = 333  # Set the maximum page number to 333\n",
    "immo_link = []\n",
    "for estate in estate_types:\n",
    "    page = 1\n",
    "    while page <= max_page:\n",
    "        url = f\"{root_url}{estate}/for-sale?countries=BE&page={page}&orderBy=relevance\"\n",
    "        req = requests.get(url)\n",
    "        print(\"Page: \", page)\n",
    "        print(\"Status Code:\", req.status_code)\n",
    "\n",
    "        soup = BeautifulSoup(req.content, 'html.parser')\n",
    "        card_results = soup.find_all('article', class_='card--result')\n",
    "\n",
    "        href_links = []\n",
    "\n",
    "        for article in card_results:\n",
    "            link = article.find('a', class_='card__title-link')\n",
    "            if link:\n",
    "                href_links.append(link['href'])\n",
    "\n",
    "        for i, link in enumerate(href_links, 1):\n",
    "            immo_link = immo_link.append(link)\n",
    "        page += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import threading\n",
    "from time import perf_counter\n",
    "\n",
    "start_time = perf_counter()\n",
    "root_url = \"https://www.immoweb.be/en/search/\"\n",
    "estate_types = ['house', 'apartment']\n",
    "max_page = 333  # Set the maximum page number to 333\n",
    "immo_link = []\n",
    "\n",
    "# Function to retrieve and parse data from a URL\n",
    "def retrieve_data(url):\n",
    "    response = requests.get(url) \n",
    "    js_content = response.text\n",
    "\n",
    "    # Extract the content within the window.dataLayer assignment using regex\n",
    "    match = re.search(r'window\\.dataLayer\\s*=\\s*(\\[.*?\\]);', js_content)\n",
    "\n",
    "    if match:\n",
    "        data = match.group(1)  # Get the matched content within the first capturing group\n",
    "        data_dict = json.loads(data)  # Parse the content into a Python dictionary\n",
    "        \n",
    "        # Replace empty values with 'none'\n",
    "        def replace_empty_with_none(obj):\n",
    "            for key, value in obj.items():\n",
    "                if isinstance(value, dict):\n",
    "                    replace_empty_with_none(value)\n",
    "                elif isinstance(value, str) and not value:\n",
    "                    obj[key] = 'none'\n",
    "        \n",
    "        replace_empty_with_none(data_dict)\n",
    "        \n",
    "        with lock:\n",
    "            data_list.append(data_dict)\n",
    "\n",
    "lock = threading.Lock() \n",
    "data_list = []\n",
    "\n",
    "for estate in estate_types:\n",
    "    page = 1\n",
    "    while page <= max_page:\n",
    "        url = f\"{root_url}{estate}/for-sale?countries=BE&page={page}&orderBy=relevance\"\n",
    "        req = requests.get(url)\n",
    "        print(\"Page:\", page)\n",
    "        print(\"Status Code:\", req.status_code)\n",
    "\n",
    "        soup = BeautifulSoup(req.content, 'html.parser')\n",
    "        card_results = soup.find_all('article', class_='card--result')\n",
    "\n",
    "        href_links = []\n",
    "\n",
    "        for article in card_results:\n",
    "            link = article.find('a', class_='card__title-link')\n",
    "            if link:\n",
    "                href_links.append(link['href'])\n",
    "\n",
    "        for link in href_links:\n",
    "            immo_link.append(link)\n",
    "            # print(\"URL:\", link)\n",
    "            # Retrieve and parse the data from the URL using threading\n",
    "            thread = threading.Thread(target=retrieve_data, args=(link,))\n",
    "            thread.start()\n",
    "\n",
    "        page += 1\n",
    "\n",
    "print(\"Scraping completed.\")\n",
    "for thread in threading.enumerate():\n",
    "    if thread != threading.current_thread():\n",
    "        thread.join()\n",
    "\n",
    "# Print the parsed data\n",
    "# for index, data_dict in enumerate(data_list):\n",
    "    # print(f\"Data from URL {immo_link[index]}:\")\n",
    "    # print(data_dict)\n",
    "    # print()\n",
    "\n",
    "print(f\"\\nTime spent inside the loop: {perf_counter() - start_time} seconds.\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import perf_counter\n",
    "from json import loads\n",
    "\n",
    "start_time = perf_counter()\n",
    "root_url = \"https://www.immoweb.be/en/search/\"\n",
    "estate_types = ['house', 'apartment']\n",
    "max_page = 333  # Set the maximum page number to 333\n",
    "immo_link = []\n",
    "data_dict =  []\n",
    "\n",
    "url = \"https://www.immoweb.be/en/classified/apartment-block/for-sale/henri-chapelle/4841/10130670\"\n",
    "# Function to retrieve and parse data from a URL\n",
    "def retrieve_data(url):\n",
    "        # print(url)\n",
    "        response = requests.get(url)\n",
    "        js_content = response.text\n",
    "        # print(js_content)\n",
    "\n",
    "        # Extract the content within the window.dataLayer assignment using regex\n",
    "        match = re.search(r'window\\.dataLayer\\s*=\\s*(\\[.*?\\]);', js_content)\n",
    "        if match:\n",
    "            data = match.group(1)  # Get the matched content within the first capturing group\n",
    "            data_dict = json.loads(data)  # Parse the content into a Python dictionary\n",
    "\n",
    "            soup = BeautifulSoup(requests.get(url).content)\n",
    "            script_text = soup.find(\"script\", text=re.compile(\"var\\s+dataLayer\")).text.split(\"= \", 1)[1]\n",
    "            json_data = loads(script_text[:script_text.find(\";\")])\n",
    "            print(\"json_data\")\n",
    "            print(json_data)\n",
    "\n",
    "            # print(data_dict)\n",
    "            # Replace empty values with 'none'\n",
    "            def replace_empty_with_none(dict_to_clean):\n",
    "                for key, value in dict_to_clean.items():\n",
    "                    if isinstance(value, dict):\n",
    "                        replace_empty_with_none(value)\n",
    "                    elif isinstance(value, str) and not value:\n",
    "                        dict_to_clean[key] = 'none'\n",
    "            replace_empty_with_none(data_dict)\n",
    "\n",
    "            with lock:\n",
    "                data_list.append(data_dict)\n",
    "retrieve_data(url)\n",
    "print(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Nepal': 'none', 'Italy': 'Rome', 'England': 'London'}\n"
     ]
    }
   ],
   "source": [
    "example_dict = {\"Nepal\": \"\", \"Italy\": \"Rome\", \"England\": \"London\"}\n",
    "\n",
    "def replace_empty_with_none(dict_to_clean):\n",
    "        for key, value in dict_to_clean.items():\n",
    "            if isinstance(value, dict):\n",
    "                replace_empty_with_none(value)\n",
    "            elif isinstance(value, str) and not value:\n",
    "                dict_to_clean[key] = 'none'\n",
    "\n",
    "\n",
    "replace_empty_with_none(example_dict)\n",
    "print(example_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_url_list():\n",
    "    for estate in estate_types:\n",
    "        page = 1\n",
    "        while page <= max_page:\n",
    "            url = f\"{root_url}{estate}/for-sale?countries=BE&page={page}&orderBy=relevance\"\n",
    "            req = requests.get(url)\n",
    "            soup = BeautifulSoup(req.content, 'html.parser')\n",
    "            card_results = soup.find_all('article', class_='card--result')\n",
    "            immo_links = []\n",
    "            for article in card_results:\n",
    "                link = article.find('a', class_='card__title-link')\n",
    "                if link:\n",
    "                    immo_links.append(link['href'])\n",
    "            page += 1\n",
    "    print(immo_links)\n",
    "\n",
    "get_url_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m immo_links \u001b[39m=\u001b[39m []\n\u001b[1;32m     50\u001b[0m \u001b[39mwith\u001b[39;00m Pool() \u001b[39mas\u001b[39;00m pool:\n\u001b[0;32m---> 51\u001b[0m     immo_links \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39;49mmap(get_url_list, estate_types)\n\u001b[1;32m     52\u001b[0m     \u001b[39mprint\u001b[39m(immo_links)\n\u001b[1;32m     54\u001b[0m immo_dicts \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m, func, iterable, chunksize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[39m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[39m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_async(func, iterable, mapstar, chunksize)\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    769\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event\u001b[39m.\u001b[39;49mwait(timeout)\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    608\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import concurrent.futures\n",
    "from multiprocessing import Pool\n",
    "import json\n",
    "from time import perf_counter\n",
    "\n",
    "root_url = \"https://www.immoweb.be/en/search/\"\n",
    "estate_types = ['house', 'apartment']\n",
    "max_page = 333  # Set the maximum page number to 333\n",
    "\n",
    "def get_url_list(estate):\n",
    "    immo_links = []\n",
    "    page = 1\n",
    "    while page <= max_page:\n",
    "        url = f\"{root_url}{estate}/for-sale?countries=BE&page={page}&orderBy=relevance\"\n",
    "        req = requests.get(url)\n",
    "        soup = BeautifulSoup(req.content, 'html.parser')\n",
    "        card_results = soup.find_all('article', class_='card--result')\n",
    "        for article in card_results:\n",
    "            link = article.find('a', class_='card__title-link')\n",
    "            if link:\n",
    "                immo_links.append(link['href'])\n",
    "        page += 1\n",
    "    return immo_links\n",
    "\n",
    "def get_immo_dict(link):\n",
    "    req = requests.get(link)\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "    script_tags = soup.find_all('script')\n",
    "    second_script = script_tags[1]\n",
    "    script_content = second_script.string\n",
    "    new_script_content = script_content.split('\"classified\": ')[1]\n",
    "    new_new_cont = new_script_content.split(\"\"\",\n",
    "                                    \"customer\": \"\"\")[0]\n",
    "    dict1 = json.loads(new_new_cont)\n",
    "    return dict1\n",
    "\n",
    "def replace_empty_with_none(dict_to_clean):\n",
    "    for key, value in dict_to_clean.items():\n",
    "        if isinstance(value, dict):\n",
    "            replace_empty_with_none(value)\n",
    "        elif isinstance(value, str) and not value:\n",
    "            dict_to_clean[key] = None\n",
    "    return dict_to_clean\n",
    "\n",
    "start_time = perf_counter()\n",
    "\n",
    "immo_links = []\n",
    "with Pool() as pool:\n",
    "    immo_links = pool.map(get_url_list, estate_types)\n",
    "    print(immo_links)\n",
    "\n",
    "immo_dicts = []\n",
    "with Pool() as pool:\n",
    "    results = pool.map(get_immo_dict, immo_links)\n",
    "    print(immo_dicts)\n",
    "    for result in results:\n",
    "        result = replace_empty_with_none(result)\n",
    "        immo_dicts.append(result)\n",
    "\n",
    "with open('immo_dump.json', 'w') as outfile:\n",
    "    json.dump(immo_dicts, outfile, indent=4)\n",
    "\n",
    "print(\"Scraping completed\")\n",
    "print(f\"\\nTime spent inside the loop: {perf_counter() - start_time} seconds.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function :\n",
    "    get_url_list\n",
    "        list(url_list)\n",
    "\n",
    "    get_immo_dict(url_list)\n",
    "        list(immo_dict) or json\n",
    "    \n",
    "    replace_empty_with_none(immo_dict)\n",
    "        list(cleaned_immo_dict) or json\n",
    "        \n",
    "    get_csv_file(immo_dict)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
