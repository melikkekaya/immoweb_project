{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "root_url = \"https://www.immoweb.be/en/search/\"\n",
    "distribution_type = \"BUY\"\n",
    "estate_type = \"HOUSE\"\n",
    "# need to change in  a list \n",
    "# estate_type =['house', 'apartment'] \n",
    "country = \"BE\"\n",
    "\n",
    "search_url = f\"{root_url}{distribution_type}/{estate_type}?countries={country}\"\n",
    "print(search_url)\n",
    "response = requests.get(search_url)\n",
    "html_content = response.text\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "house_listings = soup.find_all(\"a\", class_=\"card__title-link\")\n",
    "house_urls = [listing[\"href\"] for listing in house_listings]\n",
    "for url in house_urls:\n",
    "    print(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\"house/for-sale?countries=BE&priceType=SALE_PRICE&page=1&orderBy=relevance\"\n",
    "\"https://www.immoweb.be/en/search/house/for-sale?countries=BE&priceType=SALE_PRICE&page=333&orderBy=relevance\"\n",
    "root_url = \"https://www.immoweb.be/en/search/\"\n",
    "estate_types = ['house', 'apartment']\n",
    "max_page = 333  # Set the maximum page number to 333\n",
    "immo_link = []\n",
    "for estate in estate_types:\n",
    "    page = 1\n",
    "    while page <= max_page:\n",
    "        url = f\"{root_url}{estate}/for-sale?countries=BE&page={page}&orderBy=relevance\"\n",
    "        req = requests.get(url)\n",
    "        print(\"Page: \", page)\n",
    "        print(\"Status Code:\", req.status_code)\n",
    "\n",
    "        soup = BeautifulSoup(req.content, 'html.parser')\n",
    "        card_results = soup.find_all('article', class_='card--result')\n",
    "\n",
    "        href_links = []\n",
    "\n",
    "        for article in card_results:\n",
    "            link = article.find('a', class_='card__title-link')\n",
    "            if link:\n",
    "                href_links.append(link['href'])\n",
    "\n",
    "        for i, link in enumerate(href_links, 1):\n",
    "            immo_link = immo_link.append(link)\n",
    "            print(i, link)\n",
    "\n",
    "\n",
    "        page += 1\n",
    "\n",
    "print(\"Scraping completed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import threading\n",
    "from time import perf_counter\n",
    "\n",
    "start_time = perf_counter()\n",
    "root_url = \"https://www.immoweb.be/en/search/\"\n",
    "estate_types = ['house', 'apartment']\n",
    "max_page = 333  # Set the maximum page number to 333\n",
    "immo_link = []\n",
    "\n",
    "# Function to retrieve and parse data from a URL\n",
    "def retrieve_data(url):\n",
    "    response = requests.get(url) \n",
    "    js_content = response.text\n",
    "\n",
    "    # Extract the content within the window.dataLayer assignment using regex\n",
    "    match = re.search(r'window\\.dataLayer\\s*=\\s*(\\[.*?\\]);', js_content)\n",
    "\n",
    "    if match:\n",
    "        data = match.group(1)  # Get the matched content within the first capturing group\n",
    "        data_dict = json.loads(data)  # Parse the content into a Python dictionary\n",
    "        \n",
    "        # Replace empty values with 'none'\n",
    "        def replace_empty_with_none(obj):\n",
    "            for key, value in obj.items():\n",
    "                if isinstance(value, dict):\n",
    "                    replace_empty_with_none(value)\n",
    "                elif isinstance(value, str) and not value:\n",
    "                    obj[key] = 'none'\n",
    "        \n",
    "        replace_empty_with_none(data_dict)\n",
    "        \n",
    "        with lock:\n",
    "            data_list.append(data_dict)\n",
    "\n",
    "lock = threading.Lock() \n",
    "data_list = []\n",
    "\n",
    "for estate in estate_types:\n",
    "    page = 1\n",
    "    while page <= max_page:\n",
    "        url = f\"{root_url}{estate}/for-sale?countries=BE&page={page}&orderBy=relevance\"\n",
    "        req = requests.get(url)\n",
    "        print(\"Page:\", page)\n",
    "        print(\"Status Code:\", req.status_code)\n",
    "\n",
    "        soup = BeautifulSoup(req.content, 'html.parser')\n",
    "        card_results = soup.find_all('article', class_='card--result')\n",
    "\n",
    "        href_links = []\n",
    "\n",
    "        for article in card_results:\n",
    "            link = article.find('a', class_='card__title-link')\n",
    "            if link:\n",
    "                href_links.append(link['href'])\n",
    "\n",
    "        for link in href_links:\n",
    "            immo_link.append(link)\n",
    "            # print(\"URL:\", link)\n",
    "            # Retrieve and parse the data from the URL using threading\n",
    "            thread = threading.Thread(target=retrieve_data, args=(link,))\n",
    "            thread.start()\n",
    "\n",
    "        page += 1\n",
    "\n",
    "print(\"Scraping completed.\")\n",
    "for thread in threading.enumerate():\n",
    "    if thread != threading.current_thread():\n",
    "        thread.join()\n",
    "\n",
    "# Print the parsed data\n",
    "# for index, data_dict in enumerate(data_list):\n",
    "    # print(f\"Data from URL {immo_link[index]}:\")\n",
    "    # print(data_dict)\n",
    "    # print()\n",
    "\n",
    "print(f\"\\nTime spent inside the loop: {perf_counter() - start_time} seconds.\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import perf_counter\n",
    "from json import loads\n",
    "\n",
    "start_time = perf_counter()\n",
    "root_url = \"https://www.immoweb.be/en/search/\"\n",
    "estate_types = ['house', 'apartment']\n",
    "max_page = 333  # Set the maximum page number to 333\n",
    "immo_link = []\n",
    "data_dict =  []\n",
    "\n",
    "url = \"https://www.immoweb.be/en/classified/apartment-block/for-sale/henri-chapelle/4841/10130670\"\n",
    "# Function to retrieve and parse data from a URL\n",
    "def retrieve_data(url):\n",
    "        # print(url)\n",
    "        response = requests.get(url)\n",
    "        js_content = response.text\n",
    "        # print(js_content)\n",
    "\n",
    "        # Extract the content within the window.dataLayer assignment using regex\n",
    "        match = re.search(r'window\\.dataLayer\\s*=\\s*(\\[.*?\\]);', js_content)\n",
    "        if match:\n",
    "            data = match.group(1)  # Get the matched content within the first capturing group\n",
    "            data_dict = json.loads(data)  # Parse the content into a Python dictionary\n",
    "\n",
    "            soup = BeautifulSoup(requests.get(url).content)\n",
    "            script_text = soup.find(\"script\", text=re.compile(\"var\\s+dataLayer\")).text.split(\"= \", 1)[1]\n",
    "            json_data = loads(script_text[:script_text.find(\";\")])\n",
    "            print(\"json_data\")\n",
    "            print(json_data)\n",
    "\n",
    "            # print(data_dict)\n",
    "            # Replace empty values with 'none'\n",
    "            def replace_empty_with_none(dict_to_clean):\n",
    "                for key, value in dict_to_value.items():\n",
    "                    if isinstance(value, dict):\n",
    "                        replace_empty_with_none(value)\n",
    "                    elif isinstance(value, str) and not value:\n",
    "                        dict_to_clean[key] = 'none'\n",
    "            replace_empty_with_none(data_dict)\n",
    "\n",
    "            with lock:\n",
    "                data_list.append(data_dict)\n",
    "retrieve_data(url)\n",
    "print(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function :\n",
    "    get_url_list\n",
    "        list(url_list)\n",
    "\n",
    "    get_immo_dict(url_list)\n",
    "        list(immo_dict) or json\n",
    "    \n",
    "    replace_empty_with_none(immo_dict)\n",
    "        list(cleaned_immo_dict) or json\n",
    "        \n",
    "    get_csv_file(immo_dict)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
